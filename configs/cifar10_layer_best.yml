use_pretrained: false
data:
    dataset: "CIFAR10"
    image_size: 32
    channels: 3
    logit_transform: false
    uniform_dequantization: false
    gaussian_dequantization: false
    random_flip: true
    rescaled: true
    num_workers: 4

model:
    use_time_embed: False
    type: "simple"
    block_type: "UnetBlock"
    in_channels: 3
    out_ch: 3
    ch_num: 
      - value: 32
        num: 50
      - value: 16
        num: 50
    ch_mult:
      - value: [1, 2, 2, 2]
        num: 50
      - value: [1, 2]
        num: 50

    num_res_blocks: 2
    attn_resolutions: [16, ]
    dropout: 0.1
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True
    resamp_with_conv: True
    upsamp_type: None
    input_size: 32
    output_size: 32

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    num_diffusion_timesteps: 1000
    num_block: 100
    learn_alpha: false


training:
    batch_size: 64
    n_epochs: 10000
    n_iters: 5000000
    snapshot_freq: 5000
    sample_freq: 5000
    train_type: layer

sampling:
    batch_size: 256
    ckpt: 
      - value: "2024-04-11-13-10-26"
        num: 1
      - value: "2024-04-11-13-10-26" # 2-6
        num: 5
      - value: "2024-04-07-10-21-26" # 7-11
        num: 5
      - value: "2024-04-08-15-00-42" # 12-16
        num: 5
      - value: "2024-04-10-11-34-24" # 17-21
        num: 5
      - value: "2024-04-11-13-10-26"
        num: 74
      - value: "2024-04-11-13-10-26" # 95-99
        num: 5

eval:
    num_images: 50000
    batch_size: 128
    fid_cache: /home/bingwenzhang/ddim/stats/cifar10.train.npz
    fid_use_torch: false

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.0002
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0
